{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Optimization - Johnson & Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import col_types\n",
    "\n",
    "files_processed = []\n",
    "deltas_processed = []\n",
    "if glob.glob('./data/files_processed.txt'):\n",
    "    with open('./data/files_processed.txt', 'r') as file:\n",
    "        reader = csv.reader(file, delimiter='\\n')\n",
    "        for row in reader:\n",
    "            files_processed += row\n",
    "\n",
    "if glob.glob('./data/deltas_processed.txt'):\n",
    "    with open('./data/deltas_processed.txt', 'r') as file:\n",
    "        reader = csv.reader(file, delimiter='\\n')\n",
    "        for row in reader:\n",
    "            deltas_processed += row\n",
    "            \n",
    "files_full = [file for file in glob.glob('./data/full/*.txt') if file not in files_processed]\n",
    "files_delta = [file for file in glob.glob('./data/delta/cvtJnJVisionDelta09*.txt') if file not in deltas_processed]\n",
    "files = files_full + files_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_vec = []\n",
    "\n",
    "for file in files_full:\n",
    "    full_df_vec.append(pd.read_csv(file, sep='|', dtype=col_types.types_dict, parse_dates=col_types.parse_dates))\n",
    "\n",
    "if full_df_vec:\n",
    "    full_df = pd.concat(full_df_vec)\n",
    "    full_df = full_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requisitos:\n",
    "\n",
    "Group rows by\n",
    "\n",
    "- poNumber\n",
    "- costCenter\n",
    "- primaryInternalOrder\n",
    "- profitCenter\n",
    "- generalLedgerAccount\n",
    "- needByDate\n",
    "- poEndDate\n",
    "- poStartDate\n",
    "- receivableIndicator\n",
    "- projectWbs\n",
    "- matOrSrc\n",
    "- accountingActivityCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not full_df.empty:\n",
    "    full_df = full_df.reset_index(names='id')\n",
    "    full_df['delta'] = -1\n",
    "\n",
    "    accrual = full_df.groupby(['poNumber', 'costCenter', 'primaryInternalOrder',\n",
    "    'profitCenter', 'generalLedgerAccount', 'needByDate',\n",
    "    'poEndDate', 'poStartDate', 'receivableIndicator',\n",
    "    'projectWbs', 'matOrSrc'], dropna=False)\n",
    "\n",
    "    full_df['id'] = accrual['id'].transform('min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No momento de agrupar, é necessário somar os campos que serão mesclados.\n",
    "\n",
    "`SUM(poValueInGlobalCurrency)`\n",
    "\n",
    "`SUM(poValueInLocalCurrency)`\n",
    "\n",
    "`SUM(poValueInDocCurrency)`\n",
    "\n",
    "`SUM(gdsReceiptValueInGlobalCurrency)`\n",
    "\n",
    "`SUM(gdsReceiptValueInlocalCurrency)`\n",
    "\n",
    "`SUM(gdsReceiptValueInDocCurrency)`\n",
    "\n",
    "`SUM(invoiceReceiptValueInGlobalCurrency)`\n",
    "\n",
    "`SUM(invoiceReceiptValueInLocalCurrency)`\n",
    "\n",
    "`SUM(invoiceReceiptValueInDocCurrency)`\n",
    "\n",
    "`SUM(deliverTo)`\n",
    "\n",
    "Fazer persistir colunas que somem após operação de soma, fazer como:\n",
    "`'columnName': 'first'` na propriedade do aggreggate `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not full_df.empty:\n",
    "    accrued_df = accrual.agg({\n",
    "    'id': 'min',\n",
    "    'delta': 'max',\n",
    "\n",
    "    'poNumber': 'first',\n",
    "    'costCenter': 'first',\n",
    "    'primaryInternalOrder': 'first',\n",
    "    'profitCenter': 'first',\n",
    "    'generalLedgerAccount': 'first',\n",
    "    'needByDate': 'first',\n",
    "    'poEndDate': 'first',\n",
    "    'poStartDate': 'first',\n",
    "    'receivableIndicator': 'first',\n",
    "    'projectWbs': 'first',\n",
    "    'matOrSrc': 'first',\n",
    "\n",
    "    'poName': 'first',\n",
    "    'poRequisitionerWwid': 'first',\n",
    "    'poRequisitionerWwid': 'first',\n",
    "    'poRequisitionerName': 'first',\n",
    "    'poPreparerWwid': 'first',\n",
    "    'poPreparerName': 'first',\n",
    "    'costCenterDesc': 'first',\n",
    "    'generalLedgerAccountDesc': 'first',\n",
    "    'projectWbs': 'first',\n",
    "    'supplierNumber': 'first',\n",
    "    'supplierName': 'first',\n",
    "    'supplierEmailAddress': 'first',\n",
    "    'poType': 'first',\n",
    "    'poStatus': 'first',\n",
    "    'poCloseStatus': 'first',\n",
    "    'poCreationDate': 'first',\n",
    "    'receiptDates': 'first',\n",
    "    'invoiceDates': 'first',\n",
    "    'invoicePaidStatus': 'first',\n",
    "    'transactionDate': 'first',\n",
    "    'clearingDocumentRef': 'first',\n",
    "    'clearingDateReference': 'first',\n",
    "    'localCurrencyForPoValue': 'first',\n",
    "    'documentCurrencyForPoValue': 'first',\n",
    "    'localCurrencyForGoodsReceipt': 'first',\n",
    "    'documentCurrencyForGoodsReceipt': 'first',\n",
    "    'localCurrencyForInvoiceReceipt': 'first',\n",
    "    'docCurrencyForInvoiceReceipt': 'first',\n",
    "    'poValueInGlobalCurrency': 'first',\n",
    "    'poValueInLocalCurrency': 'first',\n",
    "    'poValueInDocCurrency': 'first',\n",
    "    'gdsReceiptValueInGlobalCurrency': 'first',\n",
    "    'gdsReceiptValueInlocalCurrency': 'first',\n",
    "    'gdsReceiptValueInDocCurrency': 'first',\n",
    "    'invoiceReceiptValueInGlobalCurrency': 'first',\n",
    "    'invoiceReceiptValueInLocalCurrency': 'first',\n",
    "    'invoiceReceiptValueInDocCurrency': 'first',\n",
    "    'aribaBu': 'first',\n",
    "    'mrc': 'first',\n",
    "    'companyCode': 'first',\n",
    "    'legalEntity': 'first',\n",
    "    'fsid': 'first',\n",
    "    'region': 'first',\n",
    "    'businessArea': 'first',\n",
    "    'shipTo': 'first',\n",
    "    'deliverTo': 'first',\n",
    "    'commodityType': 'first',\n",
    "    'excludeDownpaymentRequestsForPayments': 'first',\n",
    "    'sourceSystemApprovableId': 'first',\n",
    "    'requisitionNumber': 'first',\n",
    "    'receivableIndicator': 'first',\n",
    "    'poLineNumber': 'first',\n",
    "    'splitLineNumber': 'first',\n",
    "    \n",
    "    'poValueInGlobalCurrency': 'sum',\n",
    "    'poValueInLocalCurrency': 'sum',\n",
    "    'poValueInDocCurrency': 'sum',\n",
    "    'gdsReceiptValueInGlobalCurrency': 'sum',\n",
    "    'gdsReceiptValueInlocalCurrency': 'sum',\n",
    "    'gdsReceiptValueInDocCurrency': 'sum',\n",
    "    'invoiceReceiptValueInGlobalCurrency': 'sum',\n",
    "    'invoiceReceiptValueInLocalCurrency': 'sum',\n",
    "    'invoiceReceiptValueInDocCurrency': 'sum',\n",
    "    'deliverTo': 'sum'\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    accrued_df['delta'] = accrued_df['delta'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sinaliza qual PO sofreu aggregate e qual é raw (original)\n",
    "full_df['isRaw'] = True\n",
    "accrued_df['isRaw'] = False\n",
    "\n",
    "final_df = pd.concat([full_df, accrued_df])\n",
    "#Sinaliza que todas POs são válidas, nenhuma sofreu edição ainda\n",
    "final_df['isValid'] = True\n",
    "#Salva id único da PO\n",
    "final_df['poId'] = final_df['poId'] = final_df['poNumber'] + final_df['poLineNumber'] + final_df['splitLineNumber']\n",
    "\n",
    "final_df = final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_df.empty:\n",
    "    final_df.to_csv('./data/accruedDataJnJ.csv', index=False, sep='|')\n",
    "\n",
    "    with open(\"./data/files_processed.txt\", \"w\") as txt_file:\n",
    "        for line in files_full:\n",
    "            txt_file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processando Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura de arquivos Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df_vec = []\n",
    "\n",
    "#Lê arquivos delta e armazena num vetor\n",
    "for file in files_delta:\n",
    "    df = pd.read_csv(file, sep='|', dtype=col_types.types_dict, parse_dates=col_types.parse_dates)\n",
    "    df['delta'] = -2\n",
    "    df['poId'] = df['poNumber'] + df['poLineNumber'] + df['splitLineNumber']\n",
    "    delta_df_vec.append(df)\n",
    "\n",
    "\n",
    "#Lê arquivos já accruados e seleciona as POs raws (que não são frutos de um aggregate)\n",
    "accruedDataJnJ_df = pd.read_csv('./data/accruedDataJnJ.csv', sep='|', dtype=col_types.types_dict, parse_dates=col_types.parse_dates)\n",
    "###\n",
    "#accruedDataJnJ_df['isValid'] = True #Apagar depois...\n",
    "#accruedDataJnJ_df['poId'] = accruedDataJnJ_df['poNumber'] + accruedDataJnJ_df['poLineNumber'] + accruedDataJnJ_df['splitLineNumber']\n",
    "###\n",
    "raw_df = accruedDataJnJ_df.loc[(accruedDataJnJ_df['isRaw'] == True) & (accruedDataJnJ_df['isValid'] == True)]\n",
    "\n",
    "#Preenche as POs do delta com novos IDs (começando do último)\n",
    "last_id = accruedDataJnJ_df['id'].max()\n",
    "delta_df = pd.concat(delta_df_vec)\n",
    "delta_df.insert(0, 'id', range(last_id + 1, last_id + len(delta_df) + 1))\n",
    "\n",
    "delta_raw_df_vec = [delta_df, raw_df]\n",
    "\n",
    "#Concatena todos data frames e cria coluna de poId para identificar qual PO foi editada ou se é PO nova\n",
    "if delta_df_vec:\n",
    "    delta_raw_df = pd.concat(delta_raw_df_vec)\n",
    "    delta_raw_df = delta_raw_df.reset_index(drop=True)\n",
    "    delta_raw_df['poId'] = delta_raw_df['poNumber'] + delta_raw_df['poLineNumber'] + delta_raw_df['splitLineNumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delta_df_vec:\n",
    "\n",
    "    accrual = delta_raw_df.groupby(['poNumber', 'poLineNumber', 'splitLineNumber'], dropna=False)\n",
    "\n",
    "    #Conta quantas vezes a PO se repete para ver se é nova ou editada\n",
    "    delta_raw_df['count'] = accrual['poId'].transform('count')\n",
    "\n",
    "    #POs novas\n",
    "    new_po_df = delta_raw_df.loc[(delta_raw_df['count'] == 1) & (delta_raw_df['delta'] == -2)].copy(deep=True)\n",
    "    new_po_df['delta'] = -1\n",
    "    new_po_df['isRaw'] = True\n",
    "    new_po_df['isValid'] = True\n",
    "    del new_po_df['poId']\n",
    "    del new_po_df['count']\n",
    "\n",
    "    #POs editadas\n",
    "    edited_po_df = delta_raw_df.loc[(delta_raw_df['count'] == 2) & (delta_raw_df['delta'] == -2)].copy(deep=True)\n",
    "    edited_po_df['delta'] = -1\n",
    "    edited_po_df['isRaw'] = True\n",
    "    del edited_po_df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POs editadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente identifica-se quais POs serão editadas (to_edit_po) e depois identifica as edições que serão feitas (edited_po). Ordena-se os dois data frames para que os IDs sejam mantidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POs to be edited\n",
    "to_edit_po_df = delta_raw_df.loc[(delta_raw_df['count'] == 2) & (delta_raw_df['isRaw'] == True)].copy(deep=True)\n",
    "del to_edit_po_df['count']\n",
    "\n",
    "edited_po_df.sort_values(by=['poId'], inplace=True)\n",
    "to_edit_po_df.sort_values(by=['poId'], inplace=True)\n",
    "\n",
    "edited_po_df.reset_index(drop=True, inplace=True)\n",
    "edited_po_df['id'] = to_edit_po_df['id'].reset_index(drop=True)\n",
    "\n",
    "#Invalida POs antigas, valida POs editadas\n",
    "#to_edit_po_df['isValid'] = False\n",
    "#edited_po_df['isValid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torna as POs antigas inválidas e as editadas válidas. Concatena as editadas no dataframe original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accruedDataJnJ_df.loc[(accruedDataJnJ_df.poId.isin(to_edit_po_df.poId)) & (accruedDataJnJ_df.isRaw == True),'isValid'] = False\n",
    "edited_po_df['isValid'] = True\n",
    "accruedDataJnJ_df = pd.concat([accruedDataJnJ_df, edited_po_df])\n",
    "accruedDataJnJ_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POs novas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é necessário processar as novas POs que vieram nos arquivos de delta (POs criadas recentemente). Para isso utilizamos as POs resultantes do último processamento e reprocessa-se as POs antigas com as novas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POs que já existiam\n",
    "old_po_df = accruedDataJnJ_df.loc[(accruedDataJnJ_df['delta'] == accruedDataJnJ_df['delta'].max()) &\n",
    "(accruedDataJnJ_df['isValid'] == True)]\n",
    "\n",
    "#POs que já existiam concatenadas com POs novas\n",
    "to_accrual_df = pd.concat([old_po_df, new_po_df])\n",
    "\n",
    "#Agrupando as POs\n",
    "new_accrual = to_accrual_df.groupby(['poNumber', 'costCenter', 'primaryInternalOrder',\n",
    "    'profitCenter', 'generalLedgerAccount', 'needByDate',\n",
    "    'poEndDate', 'poStartDate', 'receivableIndicator',\n",
    "    'projectWbs', 'matOrSrc'], dropna=False)\n",
    "\n",
    "#Salva o menor id dos grupos nas novas POs\n",
    "to_accrual_df['id'] = new_accrual['id'].transform('min')\n",
    "new_po_df = to_accrual_df.loc[to_accrual_df['delta'] == -1].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrega os grupos, selecionando o menor ID de cada grupo e o maior delta (talvez não seja necessário?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accrued_df = new_accrual.agg({\n",
    "    'id': 'min',\n",
    "    'isRaw': 'first',\n",
    "    'delta': 'max',\n",
    "    'isValid': 'first',\n",
    "\n",
    "    'poNumber': 'first',\n",
    "    'costCenter': 'first',\n",
    "    'primaryInternalOrder': 'first',\n",
    "    'profitCenter': 'first',\n",
    "    'generalLedgerAccount': 'first',\n",
    "    'needByDate': 'first',\n",
    "    'poEndDate': 'first',\n",
    "    'poStartDate': 'first',\n",
    "    'receivableIndicator': 'first',\n",
    "    'projectWbs': 'first',\n",
    "    'matOrSrc': 'first',\n",
    "\n",
    "    'poName': 'first',\n",
    "    'poRequisitionerWwid': 'first',\n",
    "    'poRequisitionerWwid': 'first',\n",
    "    'poRequisitionerName': 'first',\n",
    "    'poPreparerWwid': 'first',\n",
    "    'poPreparerName': 'first',\n",
    "    'costCenterDesc': 'first',\n",
    "    'generalLedgerAccountDesc': 'first',\n",
    "    'projectWbs': 'first',\n",
    "    'supplierNumber': 'first',\n",
    "    'supplierName': 'first',\n",
    "    'supplierEmailAddress': 'first',\n",
    "    'poType': 'first',\n",
    "    'poStatus': 'first',\n",
    "    'poCloseStatus': 'first',\n",
    "    'poCreationDate': 'first',\n",
    "    'receiptDates': 'first',\n",
    "    'invoiceDates': 'first',\n",
    "    'invoicePaidStatus': 'first',\n",
    "    'transactionDate': 'first',\n",
    "    'clearingDocumentRef': 'first',\n",
    "    'clearingDateReference': 'first',\n",
    "    'localCurrencyForPoValue': 'first',\n",
    "    'documentCurrencyForPoValue': 'first',\n",
    "    'localCurrencyForGoodsReceipt': 'first',\n",
    "    'documentCurrencyForGoodsReceipt': 'first',\n",
    "    'localCurrencyForInvoiceReceipt': 'first',\n",
    "    'docCurrencyForInvoiceReceipt': 'first',\n",
    "    'poValueInGlobalCurrency': 'first',\n",
    "    'poValueInLocalCurrency': 'first',\n",
    "    'poValueInDocCurrency': 'first',\n",
    "    'gdsReceiptValueInGlobalCurrency': 'first',\n",
    "    'gdsReceiptValueInlocalCurrency': 'first',\n",
    "    'gdsReceiptValueInDocCurrency': 'first',\n",
    "    'invoiceReceiptValueInGlobalCurrency': 'first',\n",
    "    'invoiceReceiptValueInLocalCurrency': 'first',\n",
    "    'invoiceReceiptValueInDocCurrency': 'first',\n",
    "    'aribaBu': 'first',\n",
    "    'mrc': 'first',\n",
    "    'companyCode': 'first',\n",
    "    'legalEntity': 'first',\n",
    "    'fsid': 'first',\n",
    "    'region': 'first',\n",
    "    'businessArea': 'first',\n",
    "    'shipTo': 'first',\n",
    "    'deliverTo': 'first',\n",
    "    'commodityType': 'first',\n",
    "    'excludeDownpaymentRequestsForPayments': 'first',\n",
    "    'sourceSystemApprovableId': 'first',\n",
    "    'requisitionNumber': 'first',\n",
    "    'receivableIndicator': 'first',\n",
    "    'poLineNumber': 'first',\n",
    "    'splitLineNumber': 'first',\n",
    "    \n",
    "    'poValueInGlobalCurrency': 'sum',\n",
    "    'poValueInLocalCurrency': 'sum',\n",
    "    'poValueInDocCurrency': 'sum',\n",
    "    'gdsReceiptValueInGlobalCurrency': 'sum',\n",
    "    'gdsReceiptValueInlocalCurrency': 'sum',\n",
    "    'gdsReceiptValueInDocCurrency': 'sum',\n",
    "    'invoiceReceiptValueInGlobalCurrency': 'sum',\n",
    "    'invoiceReceiptValueInLocalCurrency': 'sum',\n",
    "    'invoiceReceiptValueInDocCurrency': 'sum',\n",
    "    'deliverTo': 'sum'\n",
    "    })\n",
    "\n",
    "new_accrued_df['delta'] = new_accrued_df['delta'].max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As novas POs que chegaram do delta são marcadas como RAW e o resultado do accruamento é marcado com RAW = False, para indicar que passou por processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_po_df['isRaw'] = True\n",
    "new_accrued_df['isRaw'] = False\n",
    "\n",
    "final_df = pd.concat([accruedDataJnJ_df, new_po_df, new_accrued_df])\n",
    "final_df = final_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os arquivos são salvos novamente no accruedDataJnJ, agora com o processamento do delta. Note que nenhum dado é excluído/deletado, todos são mantidos. Portanto a tendência do arquivo é só aumentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_df.empty:\n",
    "    final_df.to_csv('./data/accruedDataJnJ.csv', index=False, sep='|')\n",
    "\n",
    "    with open(\"./data/deltas_processed.txt\", \"w\") as txt_file:\n",
    "        for line in files_delta:\n",
    "            txt_file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/accruedDataJnJ.csv', sep='|', dtype=col_types.types_dict, parse_dates=col_types.parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['delta'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando amostras de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui serão geradas amostras de dados para fins de estudo. Seleciona-se as 10 POs que mais se repetiram e as 10 que menos se repetiram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if not full_df.empty:\n",
    "    accruedFrames = [\n",
    "        accrued_df.sort_values(['poValueInGlobalCurrency'], ascending=False).head(2), \n",
    "        accrued_df.sort_values(['poValueInGlobalCurrency'], ascending=True).head(2)\n",
    "    ]\n",
    "\n",
    "    rawFrames = []\n",
    "\n",
    "    for accruedFrame in accruedFrames:\n",
    "        for index, row in accruedFrame.iterrows():\n",
    "            isNull = row.isnull();\n",
    "            rawFrames.append(df.loc[\n",
    "                ((df['poNumber'] == row['poNumber']) | (isNull['poNumber'] & df['poNumber'].isnull())) &\n",
    "                ((df['costCenter'] == row['costCenter'])  | (isNull['costCenter'] & df['costCenter'].isnull())) &\n",
    "                ((df['primaryInternalOrder'] == row['primaryInternalOrder'])  | (isNull['primaryInternalOrder'] & df['primaryInternalOrder'].isnull())) &\n",
    "                ((df['profitCenter'] == row['profitCenter'])  | (isNull['profitCenter'] & df['profitCenter'].isnull())) &\n",
    "                ((df['generalLedgerAccount'] == row['generalLedgerAccount'])  | (isNull['generalLedgerAccount'] & df['generalLedgerAccount'].isnull())) &\n",
    "                ((df['needByDate'] == row['needByDate'])  | (isNull['needByDate'] & df['needByDate'].isnull())) &\n",
    "                ((df['poEndDate'] == row['poEndDate'])  | (isNull['poEndDate'] & df['poEndDate'].isnull())) &\n",
    "                ((df['receivableIndicator'] == row['receivableIndicator'])  | (isNull['receivableIndicator'] & df['receivableIndicator'].isnull())) &\n",
    "                ((df['projectWbs'] == row['projectWbs'])  | (isNull['projectWbs'] & df['projectWbs'].isnull())) &\n",
    "                ((df['matOrSrc'] == row['matOrSrc'])  | (isNull['matOrSrc'] & df['matOrSrc'].isnull()))\n",
    "            ])\n",
    "\n",
    "    pd.concat(rawFrames).to_csv('./data/sampleData-rawPOs.csv', sep='|', index=False)\n",
    "    pd.concat(accruedFrames).to_csv('./data/sampleData-accruedPOs.csv', sep='|', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86392c235d1b737be9901ddfa7ccef2e50c8f87fe1a26cb4a3b26caac929b012"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
